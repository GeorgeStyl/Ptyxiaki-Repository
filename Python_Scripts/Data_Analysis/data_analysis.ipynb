{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Initialize **API** **managers** -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mMAIN CWD=/home/georger/WorkDocuments/Sxoli/Ptyxiaki/Ptyxiaki-Repository/Python_Scripts/Data_Analysis\n",
      "MAIN, LISTDIR=['Powerfleet_APIs_Management.py', 'PARAMETERS.json', '__pycache__', 'data_analysis.ipynb']\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from Powerfleet_APIs_Management import PowerFleetAPIsManager as ApiManager\n",
    "from colorama import Fore, Style\n",
    "import sys  # For printing caught exceptions\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "print(Fore.YELLOW + f\"MAIN CWD={os.getcwd()}\")\n",
    "print(f\"MAIN, LISTDIR={os.listdir()}\" + Style.RESET_ALL)\n",
    "\n",
    "# Define the ApiParameterExtractor class\n",
    "class ApiParameterExtractor:\n",
    "    def __init__(self, json_file):\n",
    "        with open(json_file, 'r') as file:\n",
    "            self.parameters = json.load(file)\n",
    "\n",
    "    def extract_parameters(self, api_type):\n",
    "        # Normalize API type to lowercase\n",
    "        api_type = api_type.lower()\n",
    "        \n",
    "        # Validate API type\n",
    "        if api_type not in self.parameters:\n",
    "            raise ValueError(f\"Invalid API type: {api_type}. Valid types are 'live_api' or 'snapshot_api'.\")\n",
    "        \n",
    "        # Return the corresponding dictionary for the given API type\n",
    "        return self.parameters[api_type]\n",
    "\n",
    "\n",
    "extractor = ApiParameterExtractor(\"PARAMETERS.json\")\n",
    "\n",
    "# Extract parameters for live_api and snapshot_api\n",
    "live_api_params         = extractor.extract_parameters(\"live_api\")\n",
    "snapshot_api_params     = extractor.extract_parameters(\"snapshot_api\")\n",
    "\n",
    "# Pass the extracted parameters to PowerFleetAPIsManager\n",
    "live_api_manager        = ApiManager(live_api_params)\n",
    "snapshot_api_manager    = ApiManager(snapshot_api_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Update mongoDB for visualizing coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mConnection to MongoDB is successful!\u001b[0m\n",
      "Proceed with operations.\n"
     ]
    }
   ],
   "source": [
    "from Powerfleet_APIs_Management import MongoDBConnector as DBConnector\n",
    "\n",
    "\n",
    "connector = DBConnector()\n",
    "if connector.check_connection():\n",
    "    print(\"Proceed with operations.\")\n",
    "else:\n",
    "    print(\"Fix connection issues before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert **JSON** to **CSV** so that **pandas** be able to use it. Also, delete the <u>JSON</u> files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's Live API\n",
      "\u001b[32mAPI Request Successful!\u001b[0m\n",
      "It's Snapshot API\n",
      "Request Body: {\n",
      "    \"vehicleId\": \"7\",\n",
      "    \"startDate\": \"2024-01-01 00:00:00\",\n",
      "    \"endDate\": \"2024-11-23 23:59:59\"\n",
      "}\n",
      "\n",
      "<Response [200]>\n",
      "API Request Successful!\n",
      "Live data saved to ../../DataSets/Live_API_DataSets.csv\n",
      "Snapshot data saved to ../../DataSets/Snapshot_API_Response_data_set.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the Live API data\n",
    "live_data = live_api_manager.get_live_data()\n",
    "\n",
    "# Prepare and get Snapshot API data\n",
    "vehicleId = snapshot_api_params[\"vehicleId\"]\n",
    "startDate = snapshot_api_params[\"startDate\"]\n",
    "endDate = snapshot_api_params[\"endDate\"]\n",
    "snapshot_data = snapshot_api_manager.get_snapshot_data(vehicleId, startDate, endDate)\n",
    "\n",
    "# Parse JSON strings into dictionaries or lists\n",
    "try:\n",
    "    # Attempt to decode Live API data\n",
    "    live_data_dict = json.loads(live_data) if isinstance(live_data, str) else live_data\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding Live JSON: {e}\")\n",
    "    live_data_dict = {}\n",
    "\n",
    "try:\n",
    "    # Attempt to decode Snapshot API data\n",
    "    snapshot_data_dict = json.loads(snapshot_data) if isinstance(snapshot_data, str) else snapshot_data\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding Snapshot JSON: {e}\")\n",
    "    snapshot_data_dict = {}\n",
    "\n",
    "# Handle Live Data: Convert to a list of dictionaries\n",
    "if isinstance(live_data_dict, dict):\n",
    "    live_data_list = [value for key, value in live_data_dict.items()]\n",
    "elif isinstance(live_data_dict, list):\n",
    "    live_data_list = live_data_dict  # Already a list of dictionaries\n",
    "else:\n",
    "    live_data_list = []  # Fallback to empty list if structure is unexpected\n",
    "\n",
    "# Handle Snapshot Data: Convert to a list of dictionaries\n",
    "if isinstance(snapshot_data_dict, dict):\n",
    "    snapshot_data_list = [value for key, value in snapshot_data_dict.items()]\n",
    "elif isinstance(snapshot_data_dict, list):\n",
    "    snapshot_data_list = snapshot_data_dict  # Already a list of dictionaries\n",
    "else:\n",
    "    snapshot_data_list = []  # Fallback to empty list if structure is unexpected\n",
    "\n",
    "# Create DataFrames from the lists\n",
    "live_df = pd.DataFrame(live_data_list)\n",
    "snapshot_df = pd.DataFrame(snapshot_data_list)\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "output_dir = \"../../DataSets\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "live_data_path = os.path.join(output_dir, \"Live_API_DataSets.csv\")\n",
    "snapshot_data_path = os.path.join(output_dir, \"Snapshot_API_Response_data_set.csv\")\n",
    "\n",
    "live_df.to_csv(live_data_path, index=False)\n",
    "snapshot_df.to_csv(snapshot_data_path, index=False)\n",
    "\n",
    "print(f\"Live data saved to {live_data_path}\")\n",
    "print(f\"Snapshot data saved to {snapshot_data_path}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Delete the file\n",
    "    file_path.unlink()\n",
    "    print(f\"{file_path} has been deleted successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"{file_path} does not exist.\")\n",
    "except PermissionError:\n",
    "    print(f\"Permission denied: Unable to delete {file_path}.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sort** <u>dates and plateId</u>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize data **dispersion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
