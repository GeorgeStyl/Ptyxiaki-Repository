{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:02.444705Z",
     "start_time": "2025-02-01T18:48:02.442519Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.neighbors import KernelDensity, BallTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import os\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# File loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:02.670714Z",
     "start_time": "2025-02-01T18:48:02.667030Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"vehicleId\", \n",
    "    \"lat\", \n",
    "    \"lng\", \n",
    "    \"dateStored\", \n",
    "    \"velocity\",\n",
    "    \"odometer\", \n",
    "    \"engineVoltage\", \n",
    "    \"dateStoredHuman\", \n",
    "    \"dateOnlyStoredHuman\",    \n",
    "    \"timeOnly\",\n",
    "    \"orientation\", \n",
    "    \"seconds_diff\", \n",
    "    \"acceleration\",\n",
    "    \"isProblem\"\n",
    "]\n",
    "\n",
    "\n",
    "input_dir   = \"../../DataSets/API_Responses/Vehicle_Data/\"\n",
    "filename    = \"all_vehicle_responses.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:03.383127Z",
     "start_time": "2025-02-01T18:48:03.325978Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vehicleId        lat        lng     dateStored  velocity  odometer  \\\n",
      "0          1  37.510833  22.385710  1717682537000       0.0       0.0   \n",
      "1          1  37.510603  22.385977  1717682540000       0.0       0.0   \n",
      "2          1  37.510640  22.385927  1717682545000       6.0       0.0   \n",
      "3          1  37.510750  22.385907  1717682551000       7.0       0.0   \n",
      "4          1  37.510877  22.385698  1717682557000      26.0       0.0   \n",
      "\n",
      "   engineVoltage      dateStoredHuman dateOnlyStoredHuman  timeOnly  \\\n",
      "0           0.28  2024-06-06 17:02:17          2024-06-06  17:02:17   \n",
      "1           0.28  2024-06-06 17:02:20          2024-06-06  17:02:20   \n",
      "2           0.28  2024-06-06 17:02:25          2024-06-06  17:02:25   \n",
      "3           0.28  2024-06-06 17:02:31          2024-06-06  17:02:31   \n",
      "4           0.28  2024-06-06 17:02:37          2024-06-06  17:02:37   \n",
      "\n",
      "  orientation  seconds_diff  acceleration  isProblem  \n",
      "0   Southeast           NaN      0.000000          0  \n",
      "1   Northwest           3.0      0.000000          0  \n",
      "2       North           5.0      0.333333          0  \n",
      "3   Northwest           6.0      0.046296          0  \n",
      "4   Northwest           6.0      0.879630          0  \n"
     ]
    }
   ],
   "source": [
    "def merge_csv_file(input_dir, filename, columns):\n",
    "    input_file = os.path.join(input_dir, filename)\n",
    "\n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileNotFoundError(f\"File '{filename}' not found in directory '{input_dir}'\")\n",
    "\n",
    "    try:\n",
    "        # Read the CSV while allowing missing columns\n",
    "        df = pd.read_csv(input_file, usecols=lambda x: x.strip() in columns, encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading '{input_file}': {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "merged_df = merge_csv_file(input_dir, filename, columns)\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set **Bounding Box** only for **Τρίπολη**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latMin = 37.49764419371479\n",
    "latMax = 37.56244081620044\n",
    "lngMin = 22.344992459074458\n",
    "lngMax = 22.521463853839485\n",
    "\n",
    "\n",
    "query_filter = 'lat >= ' +str(latMin)+' & lat <= ' + str(latMax) + ' & lng >= ' +str(lngMin)+ ' & lng <= '+str(lngMax)\n",
    "veh_data_tripoli = merged_df.query( query_filter ).copy(True)\n",
    "merged_df = veh_data_tripoli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:12.258811Z",
     "start_time": "2025-02-01T18:48:12.187714Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Density of problem points on spatial coordinates')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = merged_df\n",
    "df_danger = df[df['isProblem'] == 1]\n",
    "# df_danger = df[df['vehicleId'] == 15]\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"ticks\")\n",
    "fig, ax = plt.subplots()\n",
    "#sns.jointplot(x=df_danger['lng'], y=df_danger['lat'], kind=\"hex\", color=\"#4CB391\", ax=ax)\n",
    "ax.hexbin(x=df_danger['lng'], y=df_danger['lat'])\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.gca().yaxis.set_major_formatter(ScalarFormatter())\n",
    "plt.ticklabel_format(style='plain', axis='both')  # Disable scientific notation\n",
    "\n",
    "\n",
    "ax.set_title('Density of problem points on spatial coordinates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init DF15 (VehicleId == 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df15 = df[df[\"vehicleId\"] == 15]\n",
    "# df15 = df15.head(500)\n",
    "# df15_problem = df15[df15['isProblem'] == 1]\n",
    "# plt.plot(df15.index, df15['acceleration'])\n",
    "# plt.title('Acceleration vs Index')\n",
    "# plt.ylabel('Acceleration')\n",
    "# plt.xlabel('Index')\n",
    "# plt.scatter(df15_problem.index, df15_problem['acceleration'], color='red')\n",
    "\n",
    "# len(df15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:12.613965Z",
     "start_time": "2025-02-01T18:48:12.598846Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lng</th>\n",
       "      <th>lat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.378310</td>\n",
       "      <td>37.515229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.007049</td>\n",
       "      <td>0.006226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>22.363152</td>\n",
       "      <td>37.497893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>22.372455</td>\n",
       "      <td>37.510790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.376493</td>\n",
       "      <td>37.513002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>22.385418</td>\n",
       "      <td>37.519261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22.415382</td>\n",
       "      <td>37.533140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lng          lat\n",
       "count  1999.000000  1999.000000\n",
       "mean     22.378310    37.515229\n",
       "std       0.007049     0.006226\n",
       "min      22.363152    37.497893\n",
       "25%      22.372455    37.510790\n",
       "50%      22.376493    37.513002\n",
       "75%      22.385418    37.519261\n",
       "max      22.415382    37.533140"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_danger[['lng', 'lat']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:17.330420Z",
     "start_time": "2025-02-01T18:48:17.325963Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #### MOCK DATA #####\n",
    "#\n",
    "# data = {\n",
    "#     'lng': np.random.uniform(-180, 180, 200),\n",
    "#     'lat': np.random.uniform(-90, 90, 200)\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "# df_danger = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:17.808617Z",
     "start_time": "2025-02-01T18:48:17.802262Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:18.846543Z",
     "start_time": "2025-02-01T18:48:18.474283Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29213/2252399618.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_danger.loc[:, 'cluster'] = clusters  # Adding cluster labels to DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Extracting the coordinates\n",
    "coords = df_danger[['lng', 'lat']].values\n",
    "\n",
    "# Standardizing the data for better clustering performance\n",
    "scaler = StandardScaler()\n",
    "coords_scaled = scaler.fit_transform(coords)\n",
    "\n",
    "# Applying DBSCAN\n",
    "dbscan = DBSCAN(eps=0.02, min_samples=4)  # Adjust eps as needed\n",
    "clusters = dbscan.fit_predict(coords_scaled)\n",
    "\n",
    "df_danger.loc[:, 'cluster'] = clusters  # Adding cluster labels to DataFrame\n",
    "\n",
    "\n",
    "df_danger_cluster = df_danger[df_danger['cluster'] > -1]\n",
    "\n",
    "\n",
    "# %matplotlib inline\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_danger_cluster['lng'], df_danger_cluster['lat'], c=df_danger_cluster['cluster'], cmap='tab10', edgecolors='k', alpha=0.7)\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.title('DBSCAN Clustering of Geospatial Data')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:18.984524Z",
     "start_time": "2025-02-01T18:48:18.979522Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['vehicleId', 'lat', 'lng', 'dateStored', 'velocity', 'odometer',\n",
       "       'engineVoltage', 'dateStoredHuman', 'dateOnlyStoredHuman', 'timeOnly',\n",
       "       'orientation', 'seconds_diff', 'acceleration', 'isProblem', 'cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_danger.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vehicleId</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>dateStored</th>\n",
       "      <th>velocity</th>\n",
       "      <th>odometer</th>\n",
       "      <th>engineVoltage</th>\n",
       "      <th>seconds_diff</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>isProblem</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1.999000e+03</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>1999.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.643322</td>\n",
       "      <td>37.515229</td>\n",
       "      <td>22.378310</td>\n",
       "      <td>1.731511e+12</td>\n",
       "      <td>14.552276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.853224</td>\n",
       "      <td>3.528764</td>\n",
       "      <td>-1.071677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.171086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.178733</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>7.869791e+09</td>\n",
       "      <td>15.016579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545615</td>\n",
       "      <td>3.110904</td>\n",
       "      <td>1.126465</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.481213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.497893</td>\n",
       "      <td>22.363152</td>\n",
       "      <td>1.717683e+12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-14.722222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>37.510790</td>\n",
       "      <td>22.372455</td>\n",
       "      <td>1.728074e+12</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.621000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.111111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>37.513002</td>\n",
       "      <td>22.376493</td>\n",
       "      <td>1.730461e+12</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.853000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>-0.763889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>37.519261</td>\n",
       "      <td>22.385418</td>\n",
       "      <td>1.738578e+12</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.229000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-0.555556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>37.533140</td>\n",
       "      <td>22.415382</td>\n",
       "      <td>1.743156e+12</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.551000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>-0.505051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         vehicleId          lat          lng    dateStored     velocity  \\\n",
       "count  1999.000000  1999.000000  1999.000000  1.999000e+03  1999.000000   \n",
       "mean      7.643322    37.515229    22.378310  1.731511e+12    14.552276   \n",
       "std       4.178733     0.006226     0.007049  7.869791e+09    15.016579   \n",
       "min       1.000000    37.497893    22.363152  1.717683e+12     0.000000   \n",
       "25%       7.000000    37.510790    22.372455  1.728074e+12     6.000000   \n",
       "50%       7.000000    37.513002    22.376493  1.730461e+12    10.000000   \n",
       "75%       9.000000    37.519261    22.385418  1.738578e+12    18.000000   \n",
       "max      20.000000    37.533140    22.415382  1.743156e+12   123.000000   \n",
       "\n",
       "       odometer  engineVoltage  seconds_diff  acceleration  isProblem  \\\n",
       "count    1999.0    1999.000000   1999.000000   1999.000000     1999.0   \n",
       "mean        0.0       4.853224      3.528764     -1.071677        1.0   \n",
       "std         0.0       0.545615      3.110904      1.126465        0.0   \n",
       "min         0.0       0.000000      1.000000    -14.722222        1.0   \n",
       "25%         0.0       4.621000      2.000000     -1.111111        1.0   \n",
       "50%         0.0       4.853000      3.000000     -0.763889        1.0   \n",
       "75%         0.0       5.229000      5.000000     -0.555556        1.0   \n",
       "max         0.0       5.551000     50.000000     -0.505051        1.0   \n",
       "\n",
       "           cluster  \n",
       "count  1999.000000  \n",
       "mean      8.171086  \n",
       "std      16.481213  \n",
       "min      -1.000000  \n",
       "25%      -1.000000  \n",
       "50%      -1.000000  \n",
       "75%      11.000000  \n",
       "max      55.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_danger.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Showing convex hulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:21.258667Z",
     "start_time": "2025-02-01T18:48:21.249980Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:21.832752Z",
     "start_time": "2025-02-01T18:48:21.685594Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29213/511482417.py:7: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colors = plt.cm.get_cmap(\"tab10\", len(unique_clusters))  # Set of distinct colors for clusters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import ConvexHull\n",
    "import numpy as np\n",
    "\n",
    "def plot_convex_hulls(df, clusters, normal_df_points):\n",
    "    unique_clusters = set(clusters)\n",
    "    colors = plt.cm.get_cmap(\"tab10\", len(unique_clusters))  # Set of distinct colors for clusters\n",
    "\n",
    "    fig, ax = plt.subplots()  # Create figure and axis objects\n",
    "\n",
    "    # Plot points first for colorbar\n",
    "    for cluster in unique_clusters:\n",
    "        if cluster == -1:\n",
    "            continue  # Skip noise points\n",
    "        cluster_points = df[df['cluster'] == cluster][['lng', 'lat']].values\n",
    "        ax.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster}', c=[colors(cluster)], s=10)\n",
    "\n",
    "    ax.scatter(normal_df_points['lng'], normal_df_points['lat'], c='gray', alpha=0.5)\n",
    "\n",
    "    # Plot Convex Hulls\n",
    "    for cluster in unique_clusters:\n",
    "        if cluster == -1:\n",
    "            continue  # Skip noise points\n",
    "        cluster_points = df[df['cluster'] == cluster][['lng', 'lat']].values\n",
    "        if len(cluster_points) >= 3:  # Convex hull requires at least 3 points\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = np.append(hull.vertices, hull.vertices[0])  # Close the loop\n",
    "            ax.plot(cluster_points[hull_points, 0], cluster_points[hull_points, 1], 'r-')\n",
    "\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title('DBSCAN Clustering of Geospatial Data with Convex Hulls')\n",
    "\n",
    "    # Create colorbar using scatter points\n",
    "    cb = fig.colorbar(plt.cm.ScalarMappable(cmap=\"tab10\", norm=plt.Normalize(vmin=min(unique_clusters), vmax=max(unique_clusters))),\n",
    "                      ax=ax, label='Cluster')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_convex_hulls(df_danger_cluster, clusters, df[df['isProblem'] == 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: {'min_lng': 22.3853133, 'max_lng': 22.3874133, 'min_lat': 37.5102916, 'max_lat': 37.5114683}\n",
      "Cluster 1: {'min_lng': 22.3841633, 'max_lng': 22.3843916, 'min_lat': 37.510425, 'max_lat': 37.5105433}\n",
      "Cluster 2: {'min_lng': 22.3848866, 'max_lng': 22.3851333, 'min_lat': 37.5107683, 'max_lat': 37.5109899}\n",
      "Cluster 3: {'min_lng': 22.3844933, 'max_lng': 22.3848033, 'min_lat': 37.510815, 'max_lat': 37.5110133}\n",
      "Cluster 4: {'min_lng': 22.3854, 'max_lng': 22.3854449, 'min_lat': 37.5114433, 'max_lat': 37.5117283}\n",
      "Cluster 5: {'min_lng': 22.3832566, 'max_lng': 22.3834216, 'min_lat': 37.5120583, 'max_lat': 37.512245}\n",
      "Cluster 6: {'min_lng': 22.3759916, 'max_lng': 22.3763466, 'min_lat': 37.512855, 'max_lat': 37.5131816}\n",
      "Cluster 7: {'min_lng': 22.3749533, 'max_lng': 22.3750599, 'min_lat': 37.509825, 'max_lat': 37.509945}\n",
      "Cluster 8: {'min_lng': 22.3840833, 'max_lng': 22.3842783, 'min_lat': 37.5138016, 'max_lat': 37.5138783}\n",
      "Cluster 9: {'min_lng': 22.372835, 'max_lng': 22.3731166, 'min_lat': 37.5289633, 'max_lat': 37.5291799}\n",
      "Cluster 10: {'min_lng': 22.3719916, 'max_lng': 22.3721733, 'min_lat': 37.52493, 'max_lat': 37.5250666}\n",
      "Cluster 11: {'min_lng': 22.3722233, 'max_lng': 22.3726783, 'min_lat': 37.52681, 'max_lat': 37.5273516}\n",
      "Cluster 12: {'min_lng': 22.3695883, 'max_lng': 22.3697833, 'min_lat': 37.5115, 'max_lat': 37.51166}\n",
      "Cluster 13: {'min_lng': 22.3745449, 'max_lng': 22.3747466, 'min_lat': 37.509585, 'max_lat': 37.5098233}\n",
      "Cluster 14: {'min_lng': 22.3673316, 'max_lng': 22.36759, 'min_lat': 37.5141383, 'max_lat': 37.5143233}\n",
      "Cluster 15: {'min_lng': 22.3735466, 'max_lng': 22.3736883, 'min_lat': 37.50836, 'max_lat': 37.5085399}\n",
      "Cluster 16: {'min_lng': 22.379535, 'max_lng': 22.3796, 'min_lat': 37.5213083, 'max_lat': 37.5213983}\n",
      "Cluster 17: {'min_lng': 22.3806366, 'max_lng': 22.3808183, 'min_lat': 37.5209833, 'max_lat': 37.5210983}\n",
      "Cluster 18: {'min_lng': 22.3732616, 'max_lng': 22.3737316, 'min_lat': 37.5111483, 'max_lat': 37.51154}\n",
      "Cluster 19: {'min_lng': 22.372555, 'max_lng': 22.3727066, 'min_lat': 37.521425, 'max_lat': 37.5216549}\n",
      "Cluster 20: {'min_lng': 22.3715183, 'max_lng': 22.3721316, 'min_lat': 37.5270233, 'max_lat': 37.5274349}\n",
      "Cluster 21: {'min_lng': 22.3720699, 'max_lng': 22.3725516, 'min_lat': 37.521895, 'max_lat': 37.5221616}\n",
      "Cluster 22: {'min_lng': 22.3731849, 'max_lng': 22.37339, 'min_lat': 37.5114183, 'max_lat': 37.5115433}\n",
      "Cluster 23: {'min_lng': 22.37289, 'max_lng': 22.3729916, 'min_lat': 37.51149, 'max_lat': 37.511555}\n",
      "Cluster 24: {'min_lng': 22.3709299, 'max_lng': 22.3710433, 'min_lat': 37.5186449, 'max_lat': 37.5186983}\n",
      "Cluster 25: {'min_lng': 22.3739116, 'max_lng': 22.37446, 'min_lat': 37.5130683, 'max_lat': 37.5134366}\n",
      "Cluster 26: {'min_lng': 22.3744883, 'max_lng': 22.37585, 'min_lat': 37.5127133, 'max_lat': 37.5133983}\n",
      "Cluster 27: {'min_lng': 22.369125, 'max_lng': 22.3693816, 'min_lat': 37.5153816, 'max_lat': 37.5157916}\n",
      "Cluster 28: {'min_lng': 22.3729466, 'max_lng': 22.3733466, 'min_lat': 37.5208, 'max_lat': 37.5209883}\n",
      "Cluster 29: {'min_lng': 22.3754416, 'max_lng': 22.3755366, 'min_lat': 37.5129616, 'max_lat': 37.51309}\n",
      "Cluster 30: {'min_lng': 22.3789333, 'max_lng': 22.3792183, 'min_lat': 37.5222066, 'max_lat': 37.5222899}\n",
      "Cluster 31: {'min_lng': 22.3721666, 'max_lng': 22.3723683, 'min_lat': 37.5261566, 'max_lat': 37.5264249}\n",
      "Cluster 32: {'min_lng': 22.3697533, 'max_lng': 22.3699449, 'min_lat': 37.5192666, 'max_lat': 37.5194466}\n",
      "Cluster 33: {'min_lng': 22.3713949, 'max_lng': 22.3716216, 'min_lat': 37.5267716, 'max_lat': 37.526845}\n",
      "Cluster 34: {'min_lng': 22.3703516, 'max_lng': 22.3704949, 'min_lat': 37.5121266, 'max_lat': 37.5122316}\n",
      "Cluster 35: {'min_lng': 22.371305, 'max_lng': 22.371385, 'min_lat': 37.5125249, 'max_lat': 37.512655}\n",
      "Cluster 36: {'min_lng': 22.3720566, 'max_lng': 22.37217, 'min_lat': 37.5240233, 'max_lat': 37.5242083}\n",
      "Cluster 37: {'min_lng': 22.3728933, 'max_lng': 22.3730516, 'min_lat': 37.511175, 'max_lat': 37.5112699}\n",
      "Cluster 38: {'min_lng': 22.3763533, 'max_lng': 22.3765433, 'min_lat': 37.5128116, 'max_lat': 37.5129816}\n",
      "Cluster 39: {'min_lng': 22.3712583, 'max_lng': 22.3715249, 'min_lat': 37.5312116, 'max_lat': 37.5313683}\n",
      "Cluster 40: {'min_lng': 22.37789, 'max_lng': 22.3781666, 'min_lat': 37.5250533, 'max_lat': 37.5252999}\n",
      "Cluster 41: {'min_lng': 22.3707233, 'max_lng': 22.3710849, 'min_lat': 37.530565, 'max_lat': 37.53072}\n",
      "Cluster 42: {'min_lng': 22.3712416, 'max_lng': 22.3713666, 'min_lat': 37.5306583, 'max_lat': 37.5307449}\n",
      "Cluster 43: {'min_lng': 22.3822883, 'max_lng': 22.3824783, 'min_lat': 37.519085, 'max_lat': 37.5192216}\n",
      "Cluster 44: {'min_lng': 22.3815383, 'max_lng': 22.3817683, 'min_lat': 37.5182666, 'max_lat': 37.51846}\n",
      "Cluster 45: {'min_lng': 22.3792266, 'max_lng': 22.3797466, 'min_lat': 37.5100083, 'max_lat': 37.51049}\n",
      "Cluster 46: {'min_lng': 22.3781716, 'max_lng': 22.3783366, 'min_lat': 37.5115, 'max_lat': 37.511555}\n",
      "Cluster 47: {'min_lng': 22.379495, 'max_lng': 22.379655, 'min_lat': 37.5106266, 'max_lat': 37.51087}\n",
      "Cluster 48: {'min_lng': 22.37901, 'max_lng': 22.37926, 'min_lat': 37.5095333, 'max_lat': 37.5098316}\n",
      "Cluster 49: {'min_lng': 22.3805999, 'max_lng': 22.3808816, 'min_lat': 37.51096, 'max_lat': 37.5110449}\n",
      "Cluster 50: {'min_lng': 22.381105, 'max_lng': 22.3812733, 'min_lat': 37.51169, 'max_lat': 37.5117383}\n",
      "Cluster 51: {'min_lng': 22.3840316, 'max_lng': 22.3842216, 'min_lat': 37.5147333, 'max_lat': 37.514995}\n",
      "Cluster 52: {'min_lng': 22.3879333, 'max_lng': 22.3886466, 'min_lat': 37.515015, 'max_lat': 37.515735}\n",
      "Cluster 53: {'min_lng': 22.3797633, 'max_lng': 22.3801566, 'min_lat': 37.5110583, 'max_lat': 37.5114033}\n",
      "Cluster 54: {'min_lng': 22.38037, 'max_lng': 22.3805866, 'min_lat': 37.5205566, 'max_lat': 37.5206416}\n",
      "Cluster 55: {'min_lng': 22.3729316, 'max_lng': 22.373, 'min_lat': 37.5283533, 'max_lat': 37.5285433}\n"
     ]
    }
   ],
   "source": [
    "def get_bbox_of_clusters(df, clusters):\n",
    "    cluster_bboxes = {}\n",
    "\n",
    "    # Iterate over unique clusters (excluding -1 for noise)\n",
    "    unique_clusters = sorted(set(clusters) - {-1})  # Exclude noise points (-1)\n",
    "\n",
    "    for cluster in unique_clusters:\n",
    "        # Filter the points of the current cluster\n",
    "        cluster_points = df[df['cluster'] == cluster][['lng', 'lat']]\n",
    "        \n",
    "        # Get the minimum and maximum lng and lat for the bounding box\n",
    "        min_lng = cluster_points['lng'].min()\n",
    "        max_lng = cluster_points['lng'].max()\n",
    "        min_lat = cluster_points['lat'].min()\n",
    "        max_lat = cluster_points['lat'].max()\n",
    "\n",
    "        # Store the bounding box for the current cluster\n",
    "        cluster_bboxes[cluster] = {\n",
    "            'min_lng': min_lng,\n",
    "            'max_lng': max_lng,\n",
    "            'min_lat': min_lat,\n",
    "            'max_lat': max_lat\n",
    "        }\n",
    "\n",
    "    return cluster_bboxes\n",
    "\n",
    "cluster_bboxes = get_bbox_of_clusters(df_danger_cluster, clusters)\n",
    "\n",
    "# Display the bounding boxes for each cluster\n",
    "for cluster, bbox in cluster_bboxes.items():\n",
    "    print(f\"Cluster {cluster}: {bbox}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:28.027793Z",
     "start_time": "2025-02-01T18:48:27.999452Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Cluster: -1, Points: 1099\n",
      "\u001b[31mSkipping Cluster -1 (Noise or too few points)\u001b[0m\n",
      "Processing Cluster: 0, Points: 325\n",
      "✅ Cluster 0: Convex Hull computed, Area: 1.2971677499916861e-06\n",
      "Processing Cluster: 1, Points: 4\n",
      "✅ Cluster 1: Convex Hull computed, Area: 1.380394500025382e-08\n",
      "Processing Cluster: 2, Points: 6\n",
      "✅ Cluster 2: Convex Hull computed, Area: 2.6830109999728087e-08\n",
      "Processing Cluster: 3, Points: 6\n",
      "✅ Cluster 3: Convex Hull computed, Area: 2.68868899994919e-08\n",
      "Processing Cluster: 4, Points: 5\n",
      "✅ Cluster 4: Convex Hull computed, Area: 7.657529999450587e-09\n",
      "Processing Cluster: 5, Points: 4\n",
      "✅ Cluster 5: Convex Hull computed, Area: 1.4697500000180946e-08\n",
      "Processing Cluster: 6, Points: 15\n",
      "✅ Cluster 6: Convex Hull computed, Area: 6.910550000022119e-08\n",
      "Processing Cluster: 7, Points: 4\n",
      "✅ Cluster 7: Convex Hull computed, Area: 6.337944999887063e-09\n",
      "Processing Cluster: 8, Points: 5\n",
      "✅ Cluster 8: Convex Hull computed, Area: 5.731250000101202e-09\n",
      "Processing Cluster: 9, Points: 10\n",
      "✅ Cluster 9: Convex Hull computed, Area: 3.323655500037101e-08\n",
      "Processing Cluster: 10, Points: 5\n",
      "✅ Cluster 10: Convex Hull computed, Area: 1.4176469999940704e-08\n",
      "Processing Cluster: 11, Points: 39\n",
      "✅ Cluster 11: Convex Hull computed, Area: 1.5314685999896854e-07\n",
      "Processing Cluster: 12, Points: 6\n",
      "✅ Cluster 12: Convex Hull computed, Area: 2.190275000057524e-08\n",
      "Processing Cluster: 13, Points: 6\n",
      "✅ Cluster 13: Convex Hull computed, Area: 2.311655500029481e-08\n",
      "Processing Cluster: 14, Points: 8\n",
      "✅ Cluster 14: Convex Hull computed, Area: 2.3169640001383224e-08\n",
      "Processing Cluster: 15, Points: 5\n",
      "✅ Cluster 15: Convex Hull computed, Area: 8.309084999552751e-09\n",
      "Processing Cluster: 16, Points: 4\n",
      "✅ Cluster 16: Convex Hull computed, Area: 1.640555000000503e-09\n",
      "Processing Cluster: 17, Points: 4\n",
      "✅ Cluster 17: Convex Hull computed, Area: 5.476165000809851e-09\n",
      "Processing Cluster: 18, Points: 16\n",
      "✅ Cluster 18: Convex Hull computed, Area: 9.731136499802127e-08\n",
      "Processing Cluster: 19, Points: 5\n",
      "✅ Cluster 19: Convex Hull computed, Area: 7.723170000438024e-09\n",
      "Processing Cluster: 20, Points: 24\n",
      "✅ Cluster 20: Convex Hull computed, Area: 1.4477622500137423e-07\n",
      "Processing Cluster: 21, Points: 20\n",
      "✅ Cluster 21: Convex Hull computed, Area: 7.760288999993878e-08\n",
      "Processing Cluster: 22, Points: 4\n",
      "✅ Cluster 22: Convex Hull computed, Area: 1.2434334999140475e-08\n",
      "Processing Cluster: 23, Points: 4\n",
      "✅ Cluster 23: Convex Hull computed, Area: 2.9049999967302365e-10\n",
      "Processing Cluster: 24, Points: 4\n",
      "✅ Cluster 24: Convex Hull computed, Area: 1.2840000003244898e-09\n",
      "Processing Cluster: 25, Points: 18\n",
      "✅ Cluster 25: Convex Hull computed, Area: 1.1655158499971589e-07\n",
      "Processing Cluster: 26, Points: 54\n",
      "✅ Cluster 26: Convex Hull computed, Area: 4.621453900067553e-07\n",
      "Processing Cluster: 27, Points: 13\n",
      "✅ Cluster 27: Convex Hull computed, Area: 5.1358335001594195e-08\n",
      "Processing Cluster: 28, Points: 12\n",
      "✅ Cluster 28: Convex Hull computed, Area: 4.0691444998545065e-08\n",
      "Processing Cluster: 29, Points: 4\n",
      "✅ Cluster 29: Convex Hull computed, Area: 3.1921099999849197e-09\n",
      "Processing Cluster: 30, Points: 5\n",
      "✅ Cluster 30: Convex Hull computed, Area: 4.054195000478376e-09\n",
      "Processing Cluster: 31, Points: 7\n",
      "✅ Cluster 31: Convex Hull computed, Area: 3.0991665000805124e-08\n",
      "Processing Cluster: 32, Points: 5\n",
      "✅ Cluster 32: Convex Hull computed, Area: 1.8058419999779914e-08\n",
      "Processing Cluster: 33, Points: 5\n",
      "✅ Cluster 33: Convex Hull computed, Area: 1.0012140000318149e-08\n",
      "Processing Cluster: 34, Points: 5\n",
      "✅ Cluster 34: Convex Hull computed, Area: 5.8886100000169105e-09\n",
      "Processing Cluster: 35, Points: 4\n",
      "✅ Cluster 35: Convex Hull computed, Area: 3.5547199999896266e-09\n",
      "Processing Cluster: 36, Points: 4\n",
      "✅ Cluster 36: Convex Hull computed, Area: 1.1018889999667059e-08\n",
      "Processing Cluster: 37, Points: 4\n",
      "✅ Cluster 37: Convex Hull computed, Area: 7.505724999838183e-09\n",
      "Processing Cluster: 38, Points: 6\n",
      "✅ Cluster 38: Convex Hull computed, Area: 1.4986250000865363e-08\n",
      "Processing Cluster: 39, Points: 7\n",
      "✅ Cluster 39: Convex Hull computed, Area: 1.4087359999592969e-08\n",
      "Processing Cluster: 40, Points: 10\n",
      "✅ Cluster 40: Convex Hull computed, Area: 3.394113999970401e-08\n",
      "Processing Cluster: 41, Points: 8\n",
      "✅ Cluster 41: Convex Hull computed, Area: 1.2092220000904925e-08\n",
      "Processing Cluster: 42, Points: 4\n",
      "✅ Cluster 42: Convex Hull computed, Area: 3.5602200006937485e-09\n",
      "Processing Cluster: 43, Points: 5\n",
      "✅ Cluster 43: Convex Hull computed, Area: 1.168116499976824e-08\n",
      "Processing Cluster: 44, Points: 6\n",
      "✅ Cluster 44: Convex Hull computed, Area: 4.840109999357262e-09\n",
      "Processing Cluster: 45, Points: 36\n",
      "✅ Cluster 45: Convex Hull computed, Area: 1.5243844499784412e-07\n",
      "Processing Cluster: 46, Points: 5\n",
      "✅ Cluster 46: Convex Hull computed, Area: 4.2457800002592895e-09\n",
      "Processing Cluster: 47, Points: 9\n",
      "✅ Cluster 47: Convex Hull computed, Area: 2.2669220000095503e-08\n",
      "Processing Cluster: 48, Points: 10\n",
      "✅ Cluster 48: Convex Hull computed, Area: 3.6110554999568985e-08\n",
      "Processing Cluster: 49, Points: 10\n",
      "✅ Cluster 49: Convex Hull computed, Area: 1.1930525000593092e-08\n",
      "Processing Cluster: 50, Points: 4\n",
      "✅ Cluster 50: Convex Hull computed, Area: 3.795164999681508e-09\n",
      "Processing Cluster: 51, Points: 5\n",
      "✅ Cluster 51: Convex Hull computed, Area: 6.066749999816248e-09\n",
      "Processing Cluster: 52, Points: 74\n",
      "✅ Cluster 52: Convex Hull computed, Area: 3.092133600004082e-07\n",
      "Processing Cluster: 53, Points: 15\n",
      "✅ Cluster 53: Convex Hull computed, Area: 7.908833499803798e-08\n",
      "Processing Cluster: 54, Points: 4\n",
      "✅ Cluster 54: Convex Hull computed, Area: 7.625000005568156e-10\n",
      "Processing Cluster: 55, Points: 4\n",
      "✅ Cluster 55: Convex Hull computed, Area: 1.4155000000596107e-09\n",
      "Largest Cluster: 0\n",
      "Bounding Box: {'min_lng': 22.3853133, 'min_lat': 37.5102916, 'max_lng': 22.3874133, 'max_lat': 37.5114683}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import ConvexHull\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "def get_largest_cluster_bounding_box(df, cluster_column='cluster', coord_columns=['lng', 'lat']):\n",
    "    # Ensure required columns exist\n",
    "    if not all(col in df.columns for col in [cluster_column] + coord_columns):\n",
    "        raise ValueError(f\"DataFrame must contain columns: {cluster_column}, {coord_columns}\")\n",
    "\n",
    "    largest_area = 0\n",
    "    largest_cluster = None\n",
    "    largest_hull_points = None\n",
    "\n",
    "    # Iterate through clusters\n",
    "    for cluster, cluster_df in df.groupby(cluster_column):\n",
    "        print(f\"Processing Cluster: {cluster}, Points: {len(cluster_df)}\")  # Debug print\n",
    "\n",
    "        if cluster == -1 or len(cluster_df) < 3:  # Skip noise and small clusters\n",
    "            print(f\"\\033[31mSkipping Cluster {cluster} (Noise or too few points)\\033[0m\")\n",
    "            continue\n",
    "\n",
    "        cluster_points = cluster_df[coord_columns].values\n",
    "\n",
    "        try:\n",
    "            hull = ConvexHull(cluster_points)\n",
    "            hull_points = cluster_points[hull.vertices]\n",
    "            hull_area = hull.volume  # For 2D, 'volume' is the area\n",
    "\n",
    "            print(f\"✅ Cluster {cluster}: Convex Hull computed, Area: {hull_area}\")  # Debug print\n",
    "\n",
    "            if hull_area > largest_area:\n",
    "                largest_area = hull_area\n",
    "                largest_cluster = cluster\n",
    "                largest_hull_points = hull_points\n",
    "        except Exception as e:\n",
    "            print(f\"\\033[31m❌ Convex Hull Failed for Cluster {cluster}: {e}\\033[0m\")\n",
    "            continue\n",
    "\n",
    "    if largest_cluster is None:\n",
    "        print(\"\\033[31mNo valid clusters found.\\033[0m\")\n",
    "        return None  \n",
    "\n",
    "    # Create a bounding box using shapely\n",
    "    polygon = Polygon(largest_hull_points)\n",
    "    min_lng, min_lat, max_lng, max_lat = polygon.bounds\n",
    "\n",
    "    return {\n",
    "        'largest_cluster': largest_cluster,\n",
    "        'bounding_box': {\n",
    "            'min_lng': min_lng,\n",
    "            'min_lat': min_lat,\n",
    "            'max_lng': max_lng,\n",
    "            'max_lat': max_lat\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "result = get_largest_cluster_bounding_box(df_danger)\n",
    "if result:\n",
    "    print(f\"Largest Cluster: {result['largest_cluster']}\")\n",
    "    print(f\"Bounding Box: {result['bounding_box']}\")\n",
    "else:\n",
    "    print(\"No valid clusters found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:29.200017Z",
     "start_time": "2025-02-01T18:48:29.191447Z"
    },
    "collapsed": false
   },
   "source": [
    "## Plot Orientations with Convex Hulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Trips **every 3 seconds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['vehicleId', 'lat', 'lng', 'dateStored', 'velocity', 'odometer',\n",
      "       'engineVoltage', 'dateStoredHuman', 'dateOnlyStoredHuman', 'timeOnly',\n",
      "       'orientation', 'seconds_diff', 'acceleration', 'isProblem', 'cluster',\n",
      "       'trip_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_danger.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Δεδομένου ότι η Powerfleet είπε ότι μία από τις προυποθέσεις είναι καθε 3 seconds, έβαλα 6 seconds για να καλυψω το χρονο αποστολής έως εγγραφής στη Data Base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:29.390169Z",
     "start_time": "2025-02-01T18:48:29.385043Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       vehicleId     dateStoredHuman  seconds_diff\n",
      "6              1 2024-06-06 17:02:47           NaN\n",
      "19             1 2024-06-06 19:05:54        7387.0\n",
      "23             1 2024-06-06 19:11:40         346.0\n",
      "28             1 2024-06-07 14:15:45       68645.0\n",
      "37             1 2024-06-07 14:17:39         114.0\n",
      "...          ...                 ...           ...\n",
      "27006         20 2025-03-06 11:59:28           4.0\n",
      "27010         20 2025-03-06 12:57:34        3486.0\n",
      "27013         20 2025-03-06 12:57:48          14.0\n",
      "27025         20 2025-03-06 13:03:45         357.0\n",
      "27029         20 2025-03-06 13:09:02         317.0\n",
      "\n",
      "[1999 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29213/3159433139.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_danger['trip_id'] = df_danger.groupby('vehicleId', group_keys=False)['seconds_diff'].apply(lambda x: (x >= 6).cumsum()).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a copy of df_danger\n",
    "danger_orient = df_danger.copy()\n",
    "\n",
    "# Ensure 'dateStoredHuman' is in datetime format\n",
    "danger_orient['dateStoredHuman'] = pd.to_datetime(danger_orient['dateStoredHuman'])\n",
    "\n",
    "# Sort data by vehicleId and dateStoredHuman\n",
    "danger_orient = danger_orient.sort_values(by=['vehicleId', 'dateStoredHuman'])\n",
    "\n",
    "# Compute time differences between consecutive rows within each vehicle in seconds\n",
    "danger_orient['seconds_diff'] = danger_orient.groupby('vehicleId')['dateStoredHuman'].diff().dt.total_seconds()\n",
    "\n",
    "# Print seconds_diff for debugging\n",
    "print(danger_orient[['vehicleId', 'dateStoredHuman', 'seconds_diff']])\n",
    "\n",
    "# Assign trip_id based on a gap of 3 seconds\n",
    "df_danger['trip_id'] = df_danger.groupby('vehicleId', group_keys=False)['seconds_diff'].apply(lambda x: (x >= 6).cumsum()).reset_index(drop=True)\n",
    "\n",
    "# Fill NaN trip IDs (first row of each vehicle) with 0\n",
    "danger_orient.loc[:, 'trip_id'] = danger_orient['trip_id'].fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:: NA ΥΠΟΛΟΓΙΣΩ CONVEX HULLS ΓΙΑ ΤΑ ORIENTATIONS (ΠΡΕΠΕΙ ΝΑ ΕΙΝΑΙ ΕΝΤΟΣ 6 secs???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:29.581721Z",
     "start_time": "2025-02-01T18:48:29.577151Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:29.755180Z",
     "start_time": "2025-02-01T18:48:29.750637Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:29.911737Z",
     "start_time": "2025-02-01T18:48:29.907023Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:30.106270Z",
     "start_time": "2025-02-01T18:48:30.101129Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:30.279920Z",
     "start_time": "2025-02-01T18:48:30.275006Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:30.464254Z",
     "start_time": "2025-02-01T18:48:30.459088Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T18:48:30.648854Z",
     "start_time": "2025-02-01T18:48:30.643996Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T16:51:59.915882Z",
     "start_time": "2025-02-01T16:51:41.908673Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt# Collect coords into list\n",
    "import requests\n",
    "\n",
    "import json\n",
    "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "overpass_query = \"\"\"\n",
    "[out:json];\n",
    "area[\"ISO3166-1\"=\"DE\"][admin_level=2];\n",
    "(node[\"amenity\"=\"biergarten\"](area);way[\"amenity\"=\"biergarten\"](area);rel[\"amenity\"=\"biergarten\"](area);\n",
    ");\n",
    "out center;\n",
    "\"\"\"\n",
    "response = requests.get(overpass_url,\n",
    "                        params={'data': overpass_query})\n",
    "data = response.json()\n",
    "\n",
    "coords = []\n",
    "for element in data['elements']:\n",
    "    if element['type'] == 'node':\n",
    "        lon = element['lon']\n",
    "        lat = element['lat']\n",
    "        coords.append((lon, lat))\n",
    "    elif 'center' in element:\n",
    "        lon = element['center']['lon']\n",
    "        lat = element['center']['lat']\n",
    "        coords.append((lon, lat))# Convert coordinates into numpy array\n",
    "\n",
    "\n",
    "X = np.array(coords)\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], 'o')\n",
    "plt.title('Biergarten in Germany')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T16:57:45.289285Z",
     "start_time": "2025-02-01T16:57:15.624007Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Define bounding box (min_lon, min_lat, max_lon, max_lat)\n",
    "bbox = (-122.523, 37.704, -122.354, 37.833)  # Example: San Francisco\n",
    "\n",
    "# Example coordinate points (replace with real data)\n",
    "coords = [\n",
    "    (-122.45, 37.75),\n",
    "    (-122.40, 37.78),\n",
    "    (-122.48, 37.73)\n",
    "]\n",
    "\n",
    "# Convert to numpy array\n",
    "X = np.array(coords)\n",
    "\n",
    "# Create a figure with Cartopy\n",
    "fig, ax = plt.subplots(figsize=(6, 4), subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "\n",
    "# Set the map extent to the bounding box\n",
    "ax.set_extent([bbox[0], bbox[2], bbox[1], bbox[3]], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add roads and features\n",
    "ax.add_feature(cfeature.LAND, color=\"lightgray\")\n",
    "ax.add_feature(cfeature.OCEAN, color=\"lightblue\")\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=\":\")\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.LAKES, color=\"blue\", alpha=0.3)\n",
    "ax.add_feature(cfeature.RIVERS, color=\"blue\", alpha=0.3)\n",
    "\n",
    "# Plot points\n",
    "ax.scatter(X[:, 0], X[:, 1], color=\"red\", marker=\"o\", label=\"Points\")\n",
    "\n",
    "# Labels and title\n",
    "ax.set_title(\"Custom Location Map\")\n",
    "ax.legend()\n",
    "\n",
    "# Save the output\n",
    "output_filename = \"local_map.png\"\n",
    "plt.savefig(output_filename, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Map with points saved as {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGIS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
